# =========================================================
# Analyzer1.py
# æ³›ç”¨å‹ Â· äº‹ä»¶æ¼æ–—è‡ªåŠ¨æ´å¯Ÿï¼ˆå«ç¤ºä¾‹æ•°æ®ï¼‰- äº§å“ç‰ˆ
# âœ… N-step æ¼æ–—ï¼ˆå¯å¢å¯å‡ï¼‰
# âœ… 1 ä¸ª Breakdown ç»´åº¦ï¼ˆè‡ªåŠ¨ç­›é€‰ï¼šç”¨æˆ·å±æ€§/è®¾å¤‡ç¯å¢ƒï¼‰
# âœ… Breakdownï¼šå®Œæ•´åˆ†ç»„è¡¨ + è‡ªåŠ¨ç‚¹å Top ä¸‹æ»‘åˆ†ç»„ + é«˜äº®
# âœ… ä»ªè¡¨ç›˜ï¼ˆæ€»ä½“ï¼‰+ æŠ¥å‘Š + è¿½é—® + å¯¼å‡º
# âœ… ä¿®å¤ï¼šç¤ºä¾‹/ä¸Šä¼ å†²çªçš„ session_state æŠ¥é”™ï¼ˆradio æ•°æ®æ¥æºï¼‰
# =========================================================

import os
import requests
import pandas as pd
import duckdb
import streamlit as st
from datetime import datetime, timedelta
import random
from typing import List, Tuple, Dict, Optional

# =========================================================
# Page config
# =========================================================
st.set_page_config(page_title="äº‹ä»¶æ¼æ–—æ´å¯ŸåŠ©æ‰‹", layout="wide")

# =========================================================
# CSS
# =========================================================
st.markdown("""
<style>
html, body { overflow: auto !important; height: auto !important; }
[data-testid="stAppViewContainer"] { overflow: auto !important; }
[data-testid="stMain"], [data-testid="stSidebar"] { width: 100% !important; box-sizing: border-box !important; }
[data-testid="stMetricValue"] { font-size: 2.2rem !important; }
</style>
""", unsafe_allow_html=True)

# =========================================================
# DeepSeek Config
# =========================================================
DEEPSEEK_API_KEY = os.getenv("DEEPSEEK_API_KEY")
DEEPSEEK_BASE_URL = os.getenv("DEEPSEEK_BASE_URL", "https://api.deepseek.com")
CHAT_MODEL = "deepseek-chat"
REASONER_MODEL = "deepseek-reasoner"

# =========================================================
# Utils
# =========================================================
def deepseek_chat(messages, model, temperature=0.3):
    if not DEEPSEEK_API_KEY:
        raise RuntimeError("Missing DEEPSEEK_API_KEY")
    headers = {"Authorization": f"Bearer {DEEPSEEK_API_KEY}", "Content-Type": "application/json"}
    payload = {"model": model, "messages": messages, "temperature": temperature}
    r = requests.post(f"{DEEPSEEK_BASE_URL}/chat/completions", headers=headers, json=payload, timeout=120)
    r.raise_for_status()
    return r.json()["choices"][0]["message"]["content"]

def run_sql(con, q):
    return con.execute(q).df()

@st.cache_data(show_spinner=False)
def load_csv(file) -> pd.DataFrame:
    return pd.read_csv(file)

def sql_escape(s: str) -> str:
    return str(s).replace("'", "''")

# =========================================================
# Alert
# =========================================================
def threshold_pp(window_days: int) -> float:
    if window_days >= 30:
        return 0.15
    if window_days >= 14:
        return 0.20
    return 0.30

def level(delta_pp: float, th: float) -> str:
    if delta_pp <= -th:
        return "ğŸ”´ å¼‚å¸¸ä¸‹é™"
    if delta_pp <= -th / 2:
        return "ğŸŸ  è½»å¾®ä¸‹é™"
    if delta_pp >= th:
        return "ğŸŸ¢ æ˜æ˜¾æ”¹å–„"
    return "âšª åŸºæœ¬ç¨³å®š"

def emoji_from_level(lv: str) -> str:
    return lv.split()[0] if lv else "âšª"

def _fmt_int(x) -> str:
    try:
        return f"{int(x):,}"
    except Exception:
        return str(x)

def _pp(x) -> str:
    return f"{x:.2f} pp"

def _pct(x) -> str:
    return f"{x*100:.2f}%"

def safe_rate(num, den):
    return (num / den) if den else 0.0

# =========================================================
# Sample data
# =========================================================
def make_sample_data(n_users=400):
    base = datetime.now() - timedelta(days=14)
    rows = []
    for i in range(n_users):
        uid = f"user_{i}"
        device = random.choice(["mobile", "desktop", "tablet"])
        country = random.choice(["US", "CN", "JP", "DE"])
        t1 = base + timedelta(minutes=random.randint(0, 60 * 24 * 10))
        rows.append((uid, "page_view", int(t1.timestamp() * 1000), device, country))
        if random.random() < 0.65:
            t2 = t1 + timedelta(minutes=random.randint(1, 120))
            rows.append((uid, "click", int(t2.timestamp() * 1000), device, country))
            if random.random() < 0.45:
                t3 = t2 + timedelta(minutes=random.randint(1, 180))
                rows.append((uid, "purchase", int(t3.timestamp() * 1000), device, country))
    return pd.DataFrame(rows, columns=["user_id", "event", "timestamp", "device", "country"])

# =========================================================
# Breakdown candidate detection (user attr + device/env)
# =========================================================
ENV_KEYWORDS = ["device", "os", "browser", "platform", "app_version", "version", "ua", "user_agent"]
USER_KEYWORDS = ["country", "region", "city", "language", "lang", "gender", "age", "member", "vip", "segment", "cohort", "new_user", "is_new", "user_type"]

def classify_dim(col: str) -> str:
    c = col.lower()
    if any(k in c for k in ENV_KEYWORDS):
        return "è®¾å¤‡/ç¯å¢ƒ"
    if any(k in c for k in USER_KEYWORDS):
        return "ç”¨æˆ·å±æ€§"
    # é»˜è®¤ä¹Ÿå½“â€œç”¨æˆ·å±æ€§â€ï¼Œä½†ä¼šé€šè¿‡ç»Ÿè®¡è¿‡æ»¤æŠŠä¸åˆé€‚çš„è¸¢æ‰
    return "ç”¨æˆ·å±æ€§"

def infer_breakdown_candidates(df: pd.DataFrame, uid_col: str, evt_col: str, ts_col: str) -> List[Tuple[str, str, str]]:
    """
    è¿”å›ï¼š[(col_name, label, reason)]
    label: "ç”¨æˆ·å±æ€§" or "è®¾å¤‡/ç¯å¢ƒ"
    """
    exclude = {uid_col, evt_col, ts_col}
    candidates = []
    n = len(df)
    if n == 0:
        return candidates

    for col in df.columns:
        if col in exclude:
            continue

        s = df[col]
        # 1) æ’é™¤æ•°å€¼å‹ï¼ˆå¸¸è§çš„ price/count ç­‰ï¼‰
        if pd.api.types.is_numeric_dtype(s):
            continue

        # 2) å”¯ä¸€å€¼æ¯”ä¾‹å¤ªé«˜ï¼ˆç–‘ä¼¼ item_id / session_idï¼‰
        nunique = s.dropna().astype(str).nunique()
        unique_ratio = nunique / max(n, 1)
        if unique_ratio > 0.30:
            continue

        # 3) å•ç”¨æˆ·å¤šå€¼æ¯”ä¾‹ï¼ˆå¼ºåŠ›è¿‡æ»¤ï¼šä¸æ˜¯ç”¨æˆ·/ç¯å¢ƒå±æ€§çš„åˆ—ä¼šè¢«è¸¢ï¼‰
        tmp = df[[uid_col, col]].dropna()
        if tmp.empty:
            continue
        per_user_nunique = tmp.groupby(uid_col)[col].nunique()
        multi_rate = (per_user_nunique > 1).mean()  # æœ‰å¤šä¸ªä¸åŒå€¼çš„ç”¨æˆ·å æ¯”
        if multi_rate > 0.15:
            # å¤šå¯¹å¤šæ˜æ˜¾ï¼šå…ˆä¸æ”¯æŒ
            continue

        label = classify_dim(col)
        reason = f"nunique={nunique}, unique_ratio={unique_ratio:.2%}, multi_user_rate={multi_rate:.2%}"
        candidates.append((col, label, reason))

    # è®©æ›´â€œåƒç»´åº¦â€çš„æ’åœ¨å‰é¢ï¼šnunique è¶Šå°è¶Šé å‰
    candidates.sort(key=lambda x: df[x[0]].dropna().astype(str).nunique())
    return candidates

# =========================================================
# N-step funnel SQL (loose/strict), optional breakdown
# =========================================================
def funnel_sql_nstep(
    uid: str,
    evt: str,
    ts: str,
    steps: List[str],
    window_days: int,
    strict: bool,
    breakdown_col: Optional[str] = None,
) -> str:
    """
    Output columns:
      period ('prev'/'last'), [breakdown_col], s1..sN
    """
    n = int(window_days)
    steps_esc = [sql_escape(x) for x in steps]
    in_list = ",".join([f"'{x}'" for x in steps_esc])

    bd_select = f", {breakdown_col} AS bd" if breakdown_col else ""
    bd_group = ", bd" if breakdown_col else ""
    bd_cols_select = ", bd" if breakdown_col else ""

    if not strict:
        # Loose: per period(+bd) count distinct users per event
        select_counts = ",\n  ".join([
            f"COUNT(DISTINCT CASE WHEN e='{steps_esc[i]}' THEN u END) AS s{i+1}"
            for i in range(len(steps_esc))
        ])
        return f"""
WITH b AS (SELECT MAX(to_timestamp({ts}/1000)) m FROM events),
c AS (
  SELECT
    {uid} AS u,
    {evt} AS e,
    to_timestamp({ts}/1000) AS t
    {bd_select},
    CASE
      WHEN to_timestamp({ts}/1000) >= (SELECT m FROM b) - INTERVAL {n} DAY THEN 'last'
      WHEN to_timestamp({ts}/1000) >= (SELECT m FROM b) - INTERVAL {2*n} DAY
       AND to_timestamp({ts}/1000) <  (SELECT m FROM b) - INTERVAL {n} DAY THEN 'prev'
    END AS period
  FROM events
  WHERE {evt} IN ({in_list})
)
SELECT
  period
  {bd_cols_select},
  {select_counts}
FROM c
WHERE period IS NOT NULL
GROUP BY period{bd_group}
;
"""
    else:
        # Strict: per user(+period+bd) earliest time per step, then count sequentially
        t_cols = ",\n    ".join([
            f"MIN(CASE WHEN e='{steps_esc[i]}' THEN t END) AS t{i+1}"
            for i in range(len(steps_esc))
        ])

        # build FILTER conditions for each step count
        # s1: t1 is not null
        # s2: t1 not null AND t2 not null AND t2>=t1
        # s3: ... AND t3>=t2
        filters = []
        for i in range(len(steps_esc)):
            conds = [f"t1 IS NOT NULL"]
            for k in range(2, i + 2):
                conds.append(f"t{k} IS NOT NULL")
                conds.append(f"t{k} >= t{k-1}")
            cond = " AND ".join(conds)
            filters.append(f"COUNT(*) FILTER (WHERE {cond}) AS s{i+1}")

        select_counts = ",\n  ".join(filters)

        return f"""
WITH b AS (SELECT MAX(to_timestamp({ts}/1000)) m FROM events),
c AS (
  SELECT
    {uid} AS u,
    {evt} AS e,
    to_timestamp({ts}/1000) AS t
    {bd_select},
    CASE
      WHEN to_timestamp({ts}/1000) >= (SELECT m FROM b) - INTERVAL {n} DAY THEN 'last'
      WHEN to_timestamp({ts}/1000) >= (SELECT m FROM b) - INTERVAL {2*n} DAY
       AND to_timestamp({ts}/1000) <  (SELECT m FROM b) - INTERVAL {n} DAY THEN 'prev'
    END AS period
  FROM events
  WHERE {evt} IN ({in_list})
),
u AS (
  SELECT
    u,
    period
    {bd_cols_select},
    {t_cols}
  FROM c
  WHERE period IS NOT NULL
  GROUP BY u, period{bd_group}
)
SELECT
  period
  {bd_cols_select},
  {select_counts}
FROM u
GROUP BY period{bd_group}
;
"""

# =========================================================
# Compute rates + pp changes for a single (prev,last) pair
# =========================================================
def compute_rates_and_deltas(prev_row: pd.Series, last_row: pd.Series, step_count: int) -> Tuple[Dict[str, float], Dict[str, float]]:
    """
    returns:
      rates: {"r1_2":..., "r2_3":..., ...}
      deltas_pp: {"d1_2":..., ...} (pp)
    """
    rates_prev = {}
    rates_last = {}
    deltas = {}
    for i in range(1, step_count):
        a_prev = float(prev_row[f"s{i}"])
        b_prev = float(prev_row[f"s{i+1}"])
        a_last = float(last_row[f"s{i}"])
        b_last = float(last_row[f"s{i+1}"])
        r_prev = safe_rate(b_prev, a_prev)
        r_last = safe_rate(b_last, a_last)
        rates_prev[f"r{i}_{i+1}"] = r_prev
        rates_last[f"r{i}_{i+1}"] = r_last
        deltas[f"d{i}_{i+1}"] = (r_last - r_prev) * 100
    # overall first->last
    a_prev = float(prev_row["s1"]); b_prev = float(prev_row[f"s{step_count}"])
    a_last = float(last_row["s1"]); b_last = float(last_row[f"s{step_count}"])
    r_prev = safe_rate(b_prev, a_prev)
    r_last = safe_rate(b_last, a_last)
    rates_prev[f"r1_{step_count}"] = r_prev
    rates_last[f"r1_{step_count}"] = r_last
    deltas[f"d1_{step_count}"] = (r_last - r_prev) * 100

    # merge rates: last rates only (youé€šå¸¸å±•ç¤ºæœ¬æœŸ)
    rates = {k: rates_last[k] for k in rates_last}
    return rates, deltas

def find_worst_delta(deltas_pp: Dict[str, float]) -> Tuple[str, float]:
    # return key like "d2_3" with most negative pp
    worst_k = min(deltas_pp.keys(), key=lambda k: deltas_pp[k])
    return worst_k, float(deltas_pp[worst_k])

def pretty_step_label(dkey: str, steps: List[str]) -> str:
    # dkey like d2_3 -> Step2â†’Step3 and also event name
    parts = dkey.replace("d", "").split("_")
    i = int(parts[0]); j = int(parts[1])
    return f"Step{i}â†’Step{j}ï¼ˆ{steps[i-1]}â†’{steps[j-1]}ï¼‰"

# =========================================================
# Sidebar: data source + funnel config
# =========================================================
st.sidebar.title("æ•°æ®æ¥æº")
data_source = st.sidebar.radio(
    "é€‰æ‹©æ•°æ®æ¥æº",
    ["ğŸ§ª ä½¿ç”¨ç¤ºä¾‹æ•°æ®ï¼ˆæ— éœ€ä¸Šä¼ ï¼‰", "ğŸ“‚ ä¸Šä¼  CSVï¼ˆuser / event / timestampï¼‰"],
    index=0,
    key="data_source"
)
uploaded = None
if data_source.startswith("ğŸ“‚"):
    uploaded = st.sidebar.file_uploader("ä¸Šä¼  CSV æ–‡ä»¶", type=["csv"], key="uploaded_csv")

deep_mode = st.sidebar.toggle("æ·±åº¦åˆ†æ", value=False)
strict_mode = st.sidebar.toggle("ä¸¥æ ¼æ¼æ–—ï¼ˆæŒ‰é¡ºåºï¼‰", value=False)
window_days = st.sidebar.radio("å¯¹æ¯”å‘¨æœŸï¼ˆå¤©ï¼‰", [7, 14, 30], horizontal=True)

# =========================================================
# Main
# =========================================================
st.title("ğŸ“Š äº‹ä»¶æ¼æ–—è‡ªåŠ¨æ´å¯Ÿ")

# =========================================================
# Load data
# =========================================================
if data_source.startswith("ğŸ§ª"):
    df = make_sample_data()
    st.info("å½“å‰ä½¿ç”¨ï¼šç¤ºä¾‹æ•°æ®ï¼ˆpage_view â†’ click â†’ purchaseï¼‰ï¼Œå¹¶é™„å¸¦ device/country ä½œä¸ºç»´åº¦ç¤ºä¾‹ã€‚")
else:
    if not uploaded:
        st.info("è¯·åœ¨å·¦ä¾§ä¸Šä¼  CSV æ–‡ä»¶")
        st.stop()
    df = load_csv(uploaded)

with st.expander("æ•°æ®é¢„è§ˆ", expanded=False):
    st.dataframe(df.head(20), use_container_width=True)

# =========================================================
# Column mapping
# =========================================================
cols = {c.lower(): c for c in df.columns}
uid_col = cols.get("user_id") or cols.get("visitorid")
evt_col = cols.get("event")
ts_col = cols.get("timestamp")

if not all([uid_col, evt_col, ts_col]):
    st.error("éœ€è¦åŒ…å« user_idï¼ˆæˆ– visitoridï¼‰/ event / timestamp åˆ—")
    st.stop()

# =========================================================
# Funnel steps (N-step)
# =========================================================
st.sidebar.subheader("æ¼æ–—æ­¥éª¤ï¼ˆN-stepï¼Œå¯å¢å¯å‡ï¼‰")
events = sorted(df[evt_col].dropna().astype(str).unique().tolist())
if len(events) < 2:
    st.error("äº‹ä»¶ç§ç±»å¤ªå°‘ï¼ˆ<2ï¼‰ï¼Œæ— æ³•è¿›è¡Œæ¼æ–—åˆ†æã€‚")
    st.stop()

# init steps
if "steps" not in st.session_state:
    # å°½é‡ç”¨å‰ä¸‰ä¸ªäº‹ä»¶åšé»˜è®¤
    default = events[:3] if len(events) >= 3 else events[:2]
    st.session_state["steps"] = default

# step add/remove controls
btn_c1, btn_c2 = st.sidebar.columns(2)
with btn_c1:
    if st.button("â• æ·»åŠ ä¸€æ­¥", use_container_width=True):
        # æ–°å¢ä¸€æ­¥é»˜è®¤é€‰æœ€åä¸€ä¸ªäº‹ä»¶ï¼ˆæˆ–ç¬¬ä¸€ä¸ªï¼‰
        st.session_state["steps"].append(events[min(len(events)-1, 0)])
with btn_c2:
    if st.button("â– åˆ é™¤æœ€åä¸€æ­¥", use_container_width=True):
        if len(st.session_state["steps"]) > 2:
            st.session_state["steps"] = st.session_state["steps"][:-1]

# render step selectors
new_steps = []
for i, cur in enumerate(st.session_state["steps"]):
    idx = events.index(cur) if cur in events else 0
    val = st.sidebar.selectbox(f"Step {i+1}", events, index=idx, key=f"step_{i}")
    new_steps.append(val)

# persist
st.session_state["steps"] = new_steps
steps = [sql_escape(x) for x in st.session_state["steps"]]
step_count = len(steps)

if len(set(steps)) < len(steps):
    st.sidebar.warning("å»ºè®®æ¯ä¸€æ­¥é€‰æ‹©ä¸åŒäº‹ä»¶ï¼Œå¦åˆ™æ¼æ–—è§£é‡Šä¼šå˜å¼±ã€‚")

# =========================================================
# Breakdown selection (1 dim, auto candidates)
# =========================================================
st.sidebar.subheader("æ¼æ–—ç»´åº¦ï¼ˆBreakdownï¼‰")
cands = infer_breakdown_candidates(df, uid_col, evt_col, ts_col)

# Build options: show label + col name
bd_options = ["ä¸åˆ†ç»„ï¼ˆæ€»ä½“ï¼‰"]
bd_meta = {}  # display -> actual col
for col, label, reason in cands:
    display = f"{label}ï½œ{col}"
    bd_options.append(display)
    bd_meta[display] = col

bd_choice = st.sidebar.selectbox("é€‰æ‹©åˆ†ç»„å­—æ®µï¼ˆæœ€å¤š 1 ä¸ªï¼‰", bd_options, index=0)
breakdown_col = None
breakdown_label = None
if bd_choice != "ä¸åˆ†ç»„ï¼ˆæ€»ä½“ï¼‰":
    breakdown_col = bd_meta[bd_choice]
    breakdown_label = classify_dim(breakdown_col)
    with st.sidebar.expander("ä¸ºä»€ä¹ˆæ¨èè¿™ä¸ªå­—æ®µï¼Ÿ", expanded=False):
        # æ‰¾åˆ°åŸå› 
        reason = next((r for c, l, r in cands if c == breakdown_col), "")
        st.write(f"- ç±»å‹ï¼š{breakdown_label}")
        st.write(f"- ç»Ÿè®¡ï¼š{reason}")
        st.caption("è¯´æ˜ï¼šæˆ‘ä»¬åªæ¨èæ›´åƒâ€œç”¨æˆ·å±æ€§/è®¾å¤‡ç¯å¢ƒâ€çš„å­—æ®µï¼Œé¿å… item_id ç­‰å¤šå¯¹å¤šç»´åº¦é€ æˆæ¼æ–—ä¸å¯è§£é‡Šã€‚")

# =========================================================
# DuckDB
# =========================================================
con = duckdb.connect(":memory:")
con.register("events", df)

max_ts = con.execute(f"SELECT MAX(to_timestamp({ts_col}/1000)) FROM events").fetchone()[0]
st.caption(f"æ—¶é—´åŸºå‡†ï¼š{max_ts}ï¼ˆlast_{window_days}d vs prev_{window_days}dï¼‰")

# =========================================================
# Overall funnel (always)
# =========================================================
sql_overall = funnel_sql_nstep(uid_col, evt_col, ts_col, steps, window_days, strict_mode, breakdown_col=None)
res_all = run_sql(con, sql_overall)

# normalize period order
res_all["__o"] = res_all["period"].map({"prev": 0, "last": 1})
res_all = res_all.sort_values("__o").drop(columns="__o").reset_index(drop=True)

st.subheader("ğŸ“ˆ æ¼æ–—å¯¹æ¯”ç»“æœï¼ˆæ€»ä½“ï¼‰")
st.dataframe(res_all, use_container_width=True)

if res_all.shape[0] < 2:
    st.warning("æ€»ä½“æ²¡æœ‰å¾—åˆ° prev/last ä¸¤æœŸæ•°æ®ï¼Œå¯èƒ½æ˜¯æ•°æ®æ—¶é—´è·¨åº¦ä¸è¶³æˆ–äº‹ä»¶è¿‡å°‘ã€‚")
    st.stop()

prev_all, last_all = res_all.iloc[0], res_all.iloc[1]
rates_all, deltas_all = compute_rates_and_deltas(prev_all, last_all, step_count)
worst_k_all, worst_pp_all = find_worst_delta(deltas_all)

th = threshold_pp(int(window_days))
risk_level = level(worst_pp_all, th)

# =========================================================
# Dashboard (overall)
# =========================================================
st.subheader("ğŸš¨ è‡ªåŠ¨æ´å¯Ÿï¼ˆæ€»ä½“ï¼šæœ¬æœŸ vs ä¸ŠæœŸï¼‰")
st.caption(f"å‘¨æœŸï¼šlast_{window_days}d vs prev_{window_days}dï½œé¢„è­¦é˜ˆå€¼ï¼š{th:.2f} pp")

# KPI users
kcols = st.columns(min(3, step_count))
# åªå±•ç¤ºå‰ä¸‰ä¸ªæ­¥éª¤ç”¨æˆ·æ•°ï¼ˆé¿å…æ­¥éª¤å¤ªå¤šæŒ¤çˆ†ï¼‰
for i in range(min(3, step_count)):
    with kcols[i]:
        sname = st.session_state["steps"][i]
        st.metric(
            f"Step{i+1} ç”¨æˆ·ï¼ˆ{sname}ï¼‰",
            _fmt_int(last_all[f"s{i+1}"]),
            _fmt_int(int(last_all[f"s{i+1}"]) - int(prev_all[f"s{i+1}"]))
        )

st.divider()

# KPI conversions: åªå±•ç¤ºå‰ä¸‰ä¸ªè½¬åŒ–ï¼š1->2, 2->3, 1->N
conv_cols = st.columns(3)
with conv_cols[0]:
    if step_count >= 2:
        d = deltas_all["d1_2"]
        st.metric(
            f"{st.session_state['steps'][0]} â†’ {st.session_state['steps'][1]} è½¬åŒ–ç‡",
            _pct(rates_all["r1_2"]),
            f"{_pp(d)}  {emoji_from_level(level(d, th))}",
        )
with conv_cols[1]:
    if step_count >= 3:
        d = deltas_all["d2_3"]
        st.metric(
            f"{st.session_state['steps'][1]} â†’ {st.session_state['steps'][2]} è½¬åŒ–ç‡",
            _pct(rates_all["r2_3"]),
            f"{_pp(d)}  {emoji_from_level(level(d, th))}",
        )
with conv_cols[2]:
    d = deltas_all[f"d1_{step_count}"]
    st.metric(
        f"{st.session_state['steps'][0]} â†’ {st.session_state['steps'][-1]} æ€»è½¬åŒ–ç‡",
        _pct(rates_all[f"r1_{step_count}"]),
        f"{_pp(d)}  {emoji_from_level(level(d, th))}",
    )

# Risk summary + action hint
st.divider()
left, right = st.columns([1, 2])

worst_readable_all = pretty_step_label(worst_k_all, st.session_state["steps"])
if worst_pp_all <= -th:
    hint = "ä¼˜å…ˆå®šä½è¯¥ç¯èŠ‚ï¼šæŒ‰æ¸ é“/äººç¾¤/è®¾å¤‡/ç‰ˆæœ¬æ‹†è§£ï¼›æ£€æŸ¥è¿‘æœŸæ´»åŠ¨ã€ä»·æ ¼ã€åº“å­˜ã€æ”¯ä»˜/ä¸‹å•é“¾è·¯æ˜¯å¦å˜æ›´ã€‚"
elif worst_pp_all <= -th / 2:
    hint = "å»ºè®®åšåˆ†å±‚å¯¹æ¯”ï¼šæ‹†æ¸ é“/æ–°è€ç”¨æˆ·/å…³é”®å“ç±»/è®¾å¤‡ï¼Œåˆ¤æ–­æ˜¯å¦ç»“æ„æ€§æµé‡å˜åŒ–æˆ–ç‰¹å®šäººç¾¤å¼‚å¸¸ã€‚"
elif worst_pp_all >= th:
    hint = "å»ºè®®å¤ç›˜é©±åŠ¨å› ç´ ï¼šç¡®è®¤æå‡æ˜¯å¦æ¥è‡ªæ´»åŠ¨/ç­–ç•¥/æµé‡ç»“æ„å˜åŒ–ï¼Œå¹¶æ²‰æ·€å¯å¤ç”¨åŠ¨ä½œã€‚"
else:
    hint = "å»ºè®®æŒç»­ç›‘æ§ï¼šè‹¥è¿‘æœŸæœ‰æŠ•æ”¾/æ´»åŠ¨/ç‰ˆæœ¬æ”¹åŠ¨ï¼Œå¯åœ¨åç»­å‘¨æœŸéªŒè¯å½±å“ã€‚"

with left:
    st.markdown("### ğŸš¦ é¢„è­¦æ€»è§ˆï¼ˆæ€»ä½“ï¼‰")
    st.markdown(f"**æœ€å¤§ä¸‹é™æ­¥éª¤**ï¼š**{worst_readable_all}**ï¼ˆ{worst_pp_all:.2f} ppï¼‰")
    st.markdown(f"**çŠ¶æ€**ï¼š{risk_level}")
with right:
    st.markdown("### ğŸ§­ è¡ŒåŠ¨æç¤ºï¼ˆæ€»ä½“ï¼‰")
    st.info(hint)

# =========================================================
# Breakdown funnel (optional)
# =========================================================
breakdown_summary_text = ""
top_group_info = None  # (group_value, worst_step_label, worst_pp)

if breakdown_col:
    st.subheader(f"ğŸ§© åˆ†ç»„æ¼æ–—ï¼ˆBreakdownï¼š{breakdown_label}ï½œ{breakdown_col}ï¼‰")

    sql_bd = funnel_sql_nstep(uid_col, evt_col, ts_col, steps, window_days, strict_mode, breakdown_col=breakdown_col)
    res_bd = run_sql(con, sql_bd)

    # normalize period order
    res_bd["__o"] = res_bd["period"].map({"prev": 0, "last": 1})
    res_bd = res_bd.sort_values(["bd", "__o"]).drop(columns="__o").reset_index(drop=True)

    # å¦‚æœ bd ä¸ºç©º/ç¼ºå¤±ä¼šå‡ºç° NaNï¼Œè¿™é‡Œç»Ÿä¸€å¡«å……
    res_bd["bd"] = res_bd["bd"].astype(str).fillna("(null)")

    # ç”Ÿæˆ â€œå®Œæ•´åˆ†ç»„è¡¨â€ï¼šæ¯ä¸ª bd ä¸€è¡Œï¼Œå±•ç¤º last æœŸçš„ step users + æœ€å·® pp
    groups = []
    for g, gdf in res_bd.groupby("bd"):
        if gdf.shape[0] < 2:
            continue
        p = gdf.iloc[0]
        l = gdf.iloc[1]
        rates_g, deltas_g = compute_rates_and_deltas(p, l, step_count)
        wk, wpp = find_worst_delta(deltas_g)
        groups.append({
            "group": g,
            "worst_step": pretty_step_label(wk, st.session_state["steps"]),
            "worst_pp": round(wpp, 2),
            "status": level(wpp, th),
            **{f"last_s{i+1}": int(l[f"s{i+1}"]) for i in range(step_count)},
            **{f"last_r{i}_{i+1}": round(rates_g[f"r{i}_{i+1}"] * 100, 2) for i in range(1, step_count)},
        })

    if not groups:
        st.info("è¯¥ç»´åº¦åœ¨å½“å‰æ•°æ®ä¸‹æ— æ³•å½¢æˆå®Œæ•´çš„ prev/last ä¸¤æœŸåˆ†ç»„å¯¹æ¯”ã€‚")
    else:
        bd_table = pd.DataFrame(groups)
        bd_table = bd_table.sort_values("worst_pp").reset_index(drop=True)

        # è‡ªåŠ¨ç‚¹åæœ€å·®åˆ†ç»„
        top = bd_table.iloc[0]
        top_group_info = (top["group"], top["worst_step"], float(top["worst_pp"]))
        breakdown_summary_text = f"ä¸‹é™æœ€ä¸¥é‡çš„åˆ†ç»„æ˜¯ **{top['group']}**ï¼Œå‘ç”Ÿåœ¨ **{top['worst_step']}**ï¼ˆ{top['worst_pp']:.2f} ppï¼Œ{top['status']}ï¼‰ã€‚"

        st.markdown(f"ğŸš¨ {breakdown_summary_text}")

        # ä¸ºå±•ç¤ºå‹å¥½ï¼ŒæŒ‘é€‰å±•ç¤ºåˆ—
        show_cols = ["group", "status", "worst_pp", "worst_step"]
        # å±•ç¤ºæœ¬æœŸç”¨æˆ·æ•°ï¼ˆæœ€å¤šå‰ 4 æ­¥ï¼Œé¿å…å¤ªå®½ï¼‰
        for i in range(min(step_count, 4)):
            show_cols.append(f"last_s{i+1}")
        # å±•ç¤ºå…³é”®è½¬åŒ–ç‡ï¼ˆæœ€å¤šå‰ 3 ä¸ªè½¬åŒ–ï¼‰
        for i in range(1, min(step_count, 4)):
            show_cols.append(f"last_r{i}_{i+1}")

        bd_show = bd_table[show_cols].copy()
        # rename
        rename_map = {"group": "åˆ†ç»„", "status": "é¢„è­¦", "worst_pp": "æœ€å¤§ä¸‹æ»‘(pp)", "worst_step": "ä¸‹æ»‘æ­¥éª¤"}
        for i in range(min(step_count, 4)):
            rename_map[f"last_s{i+1}"] = f"æœ¬æœŸ Step{i+1} ç”¨æˆ·"
        for i in range(1, min(step_count, 4)):
            rename_map[f"last_r{i}_{i+1}"] = f"æœ¬æœŸ Step{i}â†’{i+1} è½¬åŒ–(%)"
        bd_show = bd_show.rename(columns=rename_map)

        # highlighter
        top_group_value = str(top["group"])
        def highlight_top(row):
            # é«˜äº® Top ä¸‹æ»‘åˆ†ç»„
            if str(row["åˆ†ç»„"]) == top_group_value:
                return ["background-color: rgba(255, 0, 0, 0.08)"] * len(row)
            return [""] * len(row)

        st.dataframe(
            bd_show.style.apply(highlight_top, axis=1),
            use_container_width=True
        )

        with st.expander("æŸ¥çœ‹åˆ†ç»„æ˜ç»†åŸå§‹è¡¨ï¼ˆå«æ›´å¤šæ­¥éª¤/æŒ‡æ ‡ï¼‰", expanded=False):
            st.dataframe(bd_table, use_container_width=True)

# =========================================================
# LLM Report (button + cache)
# =========================================================
model = REASONER_MODEL if deep_mode else CHAT_MODEL
if "report_cache" not in st.session_state:
    st.session_state.report_cache = {}

# key should include steps + breakdown selection
report_key = f"{window_days}|{strict_mode}|{deep_mode}|{','.join(st.session_state['steps'])}|bd={breakdown_col}|all={int(last_all['s1'])}"

st.subheader("ğŸ§  è¿è¥æ´å¯Ÿæ—¥æŠ¥")
colA, colB = st.columns([1, 3])
with colA:
    gen_report = st.button("ç”Ÿæˆ/åˆ·æ–°æ—¥æŠ¥", type="primary", use_container_width=True)
with colB:
    st.caption("æç¤ºï¼šåˆ‡æ¢å‘¨æœŸ/æ¼æ–—æ­¥éª¤/ç»´åº¦ä¼šå¯¼è‡´é¡µé¢é‡è·‘ï¼›æ—¥æŠ¥å»ºè®®æ‰‹åŠ¨ç”Ÿæˆï¼Œé¿å…é¢‘ç¹è°ƒç”¨æ¨¡å‹ã€‚")

if gen_report:
    # build a compact summary for prompt
    # overall deltas list
    deltas_list = []
    for i in range(1, step_count):
        dk = f"d{i}_{i+1}"
        deltas_list.append(f"- Step{i}â†’{i+1}ï¼š{deltas_all[dk]:.2f}pp")
    deltas_list.append(f"- Step1â†’{step_count}ï¼š{deltas_all[f'd1_{step_count}']:.2f}pp")

    prompt = f"""
ä½ æ˜¯äº’è”ç½‘äº§å“è¿è¥åˆ†æåŠ©æ‰‹ã€‚ä¸‹é¢æ˜¯ä¸€ä¸ªäº‹ä»¶æ¼æ–—å¯¹æ¯”ï¼ˆæŒ‰ç”¨æˆ·ï¼‰ï¼Œè¯·è¾“å‡ºä¸€ä»½â€œè¿è¥æ´å¯Ÿæ—¥æŠ¥â€ï¼ˆMarkdownï¼‰ï¼Œä¸è¦åé—®ç”¨æˆ·ã€‚

ã€æ¼æ–—å®šä¹‰ã€‘
Steps = {st.session_state['steps']}
ä¸¥æ ¼æ¼æ–—ï¼ˆé¡ºåºï¼‰={strict_mode}
å‘¨æœŸï¼šlast_{window_days}d vs prev_{window_days}d
æ—¶é—´åŸºå‡†ï¼š{max_ts}

ã€æ€»ä½“ï¼ˆä¸åˆ†ç»„ï¼‰ç»Ÿè®¡ã€‘
prev/last countsï¼š
{res_all.to_dict(orient="records")}

æ€»ä½“è½¬åŒ–å˜åŒ–ï¼ˆppï¼‰ï¼š
{chr(10).join(deltas_list)}
æœ€å¤§ä¸‹é™æ­¥éª¤ï¼š{worst_readable_all}ï¼ˆ{worst_pp_all:.2f}ppï¼Œ{risk_level}ï¼‰

ã€åˆ†ç»„æ´å¯Ÿï¼ˆå¦‚æœ‰ï¼‰ã€‘
Breakdownå­—æ®µï¼š{breakdown_col or "æ— "}
{breakdown_summary_text or "æ— åˆ†ç»„æˆ–æ— æœ‰æ•ˆåˆ†ç»„å¯¹æ¯”"}

ã€è¾“å‡ºè¦æ±‚ã€‘
å¿…é¡»åŒ…å«ï¼š
## ä¸€å¥è¯ç»“è®º
## å˜åŒ–æœ€å¤§çš„æ­¥éª¤ä¸å½±å“ï¼ˆå…ˆæ€»ä½“ï¼Œå†åˆ†ç»„ï¼‰
## å¯èƒ½åŸå› ï¼ˆå‡è®¾ï¼Œ2-4æ¡ï¼‰
## ä¸‹ä¸€æ­¥æ’æŸ¥ä¸è¿è¥åŠ¨ä½œï¼ˆæŒ‰ä¼˜å…ˆçº§ï¼Œè‡³å°‘5æ¡ï¼Œå¯æ‰§è¡Œï¼‰
"""
    try:
        with st.spinner("ç”Ÿæˆæ—¥æŠ¥ä¸­â€¦ï¼ˆæ·±åº¦åˆ†æä¼šæ›´æ…¢ï¼‰"):
            report = deepseek_chat([{"role": "user", "content": prompt}], model=model)
        st.session_state.report_cache[report_key] = report
    except Exception as e:
        st.error("æ—¥æŠ¥ç”Ÿæˆå¤±è´¥ï¼ˆå¯èƒ½æ˜¯ç½‘ç»œ/é™æµ/Key/è¶…æ—¶ï¼‰ã€‚")
        st.code(str(e))

report = st.session_state.report_cache.get(report_key)
if report:
    st.markdown(report)
else:
    st.info("ç‚¹å‡»ä¸Šé¢çš„ã€Œç”Ÿæˆ/åˆ·æ–°æ—¥æŠ¥ã€æ¥ç”Ÿæˆæ´å¯Ÿæ—¥æŠ¥ã€‚")

# =========================================================
# Chat follow-up
# =========================================================
st.subheader("ğŸ’¬ è¿›ä¸€æ­¥è¿½é—®ï¼ˆChatbotï¼‰")
with st.expander("æ‰“å¼€è¿½é—®åŒº", expanded=False):
    if "chat_history" not in st.session_state:
        st.session_state.chat_history = []

    for msg in st.session_state.chat_history:
        with st.chat_message(msg["role"]):
            st.markdown(msg["content"])

    q = st.chat_input("ä¾‹å¦‚ï¼šå“ªä¸ªåˆ†ç»„æœ€å€¼å¾—ä¼˜å…ˆæ’æŸ¥ï¼Ÿæˆ‘åº”è¯¥æ€ä¹ˆæ‹†ï¼Ÿ")
    if q:
        st.session_state.chat_history.append({"role": "user", "content": q})
        with st.chat_message("user"):
            st.markdown(q)

        context = f"""
ã€æ¼æ–—å®šä¹‰ã€‘
Steps={st.session_state['steps']}, strict={strict_mode}, window_days={window_days}

ã€æ€»ä½“ç»“æœã€‘
{res_all.to_dict(orient="records")}
æœ€å¤§ä¸‹é™æ­¥éª¤ï¼š{worst_readable_all}ï¼ˆ{worst_pp_all:.2f}ppï¼Œ{risk_level}ï¼‰
è¡ŒåŠ¨æç¤ºï¼š{hint}

ã€åˆ†ç»„ç»“æœï¼ˆå¦‚æœ‰ï¼‰ã€‘
Breakdownå­—æ®µï¼š{breakdown_col or "æ— "}
{breakdown_summary_text or "æ— åˆ†ç»„æˆ–æ— æœ‰æ•ˆåˆ†ç»„å¯¹æ¯”"}

ã€ç”¨æˆ·è¿½é—®ã€‘
{q}

è¯·å›ç­”ï¼š
- å…ˆç»™ç»“è®ºï¼ˆ1-2å¥ï¼‰
- å†ç»™ 2-3 ä¸ªå¯èƒ½åŸå› ï¼ˆå¿…é¡»æ ‡æ³¨â€œå‡è®¾â€ï¼‰
- ç»™ 5 æ¡ä¸‹ä¸€æ­¥å¯æ‰§è¡Œçš„æ‹†è§£/æ’æŸ¥å»ºè®®ï¼ˆèƒ½è½åœ°ï¼‰
- è‹¥éœ€è¦é¢å¤–å­—æ®µæˆ– SQL æ‰èƒ½ç¡®è®¤ï¼Œè¯·æ˜ç¡®å†™å‡ºâ€œéœ€è¦å“ªäº›å­—æ®µ + æ€ä¹ˆæŸ¥â€
"""
        try:
            with st.chat_message("assistant"):
                with st.spinner("ç”Ÿæˆå›ç­”ä¸­â€¦"):
                    ans = deepseek_chat([{"role": "user", "content": context}], model=CHAT_MODEL, temperature=0.3)
                st.markdown(ans)
            st.session_state.chat_history.append({"role": "assistant", "content": ans})
        except Exception as e:
            st.error("è¿½é—®å¤±è´¥ï¼ˆå¯èƒ½æ˜¯ç½‘ç»œ/é™æµ/Key/è¶…æ—¶ï¼‰ã€‚")
            st.code(str(e))

# =========================================================
# Export
# =========================================================
st.subheader("ğŸ“¥ å¯¼å‡ºæ—¥æŠ¥")
md = f"""# äº‹ä»¶æ¼æ–—æ´å¯Ÿæ—¥æŠ¥

- Steps: {st.session_state['steps']}
- å‘¨æœŸ: last_{window_days}d vs prev_{window_days}d
- ä¸¥æ ¼æ¼æ–—: {strict_mode}
- Breakdown: {breakdown_col or "æ— "}
- é¢„è­¦é˜ˆå€¼: {th:.2f} pp
- æ—¶é—´åŸºå‡†: {max_ts}

## æ€»ä½“é¢„è­¦
- æœ€å¤§ä¸‹é™æ­¥éª¤ï¼š{worst_readable_all}ï¼ˆ{worst_pp_all:.2f} ppï¼Œ{risk_level}ï¼‰
- è¡ŒåŠ¨æç¤ºï¼š{hint}

## åˆ†ç»„æ´å¯Ÿï¼ˆå¦‚æœ‰ï¼‰
{breakdown_summary_text or "æ— "}

## è¿è¥æ´å¯Ÿæ—¥æŠ¥
{report or "ï¼ˆå°šæœªç”Ÿæˆæ—¥æŠ¥ï¼Œè¯·å…ˆç‚¹å‡»â€œç”Ÿæˆ/åˆ·æ–°æ—¥æŠ¥â€ï¼‰"}
"""
fname = f"äº‹ä»¶æ¼æ–—æ´å¯Ÿ_{window_days}d.md"
st.download_button("â¬‡ï¸ ä¸‹è½½ Markdown æ—¥æŠ¥", md.encode("utf-8"), fname)
